{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "From_Expressions_to_Optimization_and_Machine_Learning_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "3fc669b96b07381b454c4ddad74b139b",
          "grade": false,
          "grade_id": "cell-4923f4be6144eebe",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "P_HCgHsBPFup",
        "colab_type": "text"
      },
      "source": [
        "## ML in a nutshell\n",
        "\n",
        "Optimization, and machine learning, are intimately connected.  At a very coarse level, ML works as follows. \n",
        "\n",
        "First, you come up somehow with a very complicated model $\\vec{y} = M(\\vec{x}, \\vec{\\theta})$, which computes an output $\\vec{y}$ as a function of an input $\\vec{x}$ and of a vector of parameters $\\vec{\\theta}$.   In general, $\\vec{x}$, $\\vec{y}$, and $\\vec{\\theta}$ are vectors, as the model has multiple inputs, multiple outputs, and several parameters.  The model $M$ needs to be complicated, because only complicated models can represent complicated phenomena; for instance, $M$ can be a multi-layer neural net with parameters $\\vec{\\theta} = [\\theta_1, \\ldots, \\theta_k]$, where $k$ is the number of parameters of the model. \n",
        "\n",
        "Second, you come up with a notion of _loss_ $L$, that is, how badly the model is doing.  For instance, if you have a list of inputs $\\vec{x}_1, \\ldots, \\vec{x}_n$, and a set of desired outputs $\\vec{y}_1, \\ldots, \\vec{y}_m$, you can use as loss: \n",
        "\n",
        "$$\n",
        "L(\\vec{\\theta}) = \\sum_{i=1}^n |\\!|\\vec{y}_i - M(\\vec{x}_i, \\vec{\\theta})|\\!| \\; .\n",
        "$$\n",
        "\n",
        "Here, we wrote $L(\\vec{\\theta})$ because, once the inputs $\\vec{x}_1, \\ldots, \\vec{x}_n$ and the desired outputs $\\vec{y}_1, \\ldots, \\vec{y}_n$ are chosen, the loss $L$ depends on $\\vec{\\theta}$. \n",
        "\n",
        "Once the loss is chosen, you decrease it, by computing its _gradient_ with respect to $\\vec{\\theta}$.  Remembering that $\\vec{\\theta} = [\\theta_1, \\ldots, \\theta_k]$,\n",
        "\n",
        "$$\n",
        "\\nabla_\\vec{\\theta} L = \\left[ \\frac{\\partial L}{\\partial \\theta_1}, \\ldots, \n",
        "    \\frac{\\partial L}{\\partial \\theta_k} \\right] \\; .\n",
        "$$\n",
        "\n",
        "The gradient is a vector that indicates how to tweak $\\vec{\\theta}$ to decrease the loss.  You then choose a small _step size_ $\\delta$, and you update $\\vec{\\theta}$ via $\\vec{\\theta} := \\vec{\\theta} - \\delta \\nabla_\\vec{\\theta} L$.  This makes the loss a little bit smaller, and the model a little bit better.  If you repeat this step many times, the model will hopefully get (a good bit) better. \n",
        "\n",
        "## Autogradient\n",
        "\n",
        "The key to _pleasant_ ML is to focus on building the model $M$ in a way that is sufficiently expressive, and on choosing a loss $L$ that is helpful in guiding the optimization.  The computation of the gradient is done automatically for you.  This capability, called _autogradient_, is implemented in ML frameworks such as [Tensorflow](https://www.tensorflow.com), [Keras](https://keras.io), and [PyTorch](https://pytorch.org).  \n",
        "\n",
        "It is possible to use these advanced ML libraries without ever knowing what is under the hood, and how autogradient works.  Here, we will insted dive in, and implement autogradient.  \n",
        "\n",
        "Building a model $M$ corresponds to building an expression with inputs $\\vec{x}$, $\\vec{\\theta}$.  We will provide a representaton for expressions that enables both the calculation of the expression value, and the differentiation with respect to any of the inputs.  This will enable us to implement autogradient.  On the basis of this, we will be able to implement a simple ML framework. \n",
        "\n",
        "We say we, but we mean you.  _You_ will implement it; we will just provide guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "57509aab9f267439272653b6a2a93d0c",
          "grade": false,
          "grade_id": "cell-ba50e86bd865c21f",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "7m0-lPBOPFur",
        "colab_type": "text"
      },
      "source": [
        "## Expressions with autogradient\n",
        "\n",
        "Our main task will be to implement a class `Expr` that represents expressions with autogradient.  \n",
        "\n",
        "### Implementing expressions\n",
        "\n",
        "We will have `Expr` be the abstract class of a generic expression, and `Plus`, `Multiply`, and so on, be derived classes representing expression with given top-level operators.  The constructor takes the children node.  The code for the constructor, and the code to create addition expressions, is as follows. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "91f7290ef8bb3001209717bb1f77f09f",
          "grade": false,
          "grade_id": "cell-3700bc9f6e738800",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "A_o5_VS9PFut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Expr(object):\n",
        "\n",
        "    def __init__(self, *args):\n",
        "        \"\"\"Initializes an expression node, with a given list of children \n",
        "        expressions.\"\"\"\n",
        "        self.children = args\n",
        "        self.value = None # The value of the expression. \n",
        "        self.values = None # The values of the child expressions.\n",
        "\n",
        "    def __add__(self, other):\n",
        "        \"\"\"Constructs the sum of two expressions.\"\"\"\n",
        "        return Plus(self, other)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1176b5bcf497b2f32b2a2efba193242b",
          "grade": false,
          "grade_id": "cell-e219b32f7c82ff9",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "xaL-L2sJPFu0",
        "colab_type": "text"
      },
      "source": [
        "The code for the `Plus` class, initially, is empty; no `Expr` methods are over-ridden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "3761a5757c8c528bea1857c0fd952595",
          "grade": false,
          "grade_id": "cell-83b344280f9e96bc",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "TuKVcGAuPFu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Plus(Expr):\n",
        "    \"\"\"An addition expression.\"\"\"\n",
        "\n",
        "    pass"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "bee4e25489ff1f6e0341d9169104c5ec",
          "grade": false,
          "grade_id": "cell-42d81112e640013b",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "EG7tzdKkPFu6",
        "colab_type": "text"
      },
      "source": [
        "To construct expressions, we need one more thing.  So far, if we write things like `2 + 3`, Python will just consider these as expressions involving numbers, and compute their value.  To write _symbolic_ expressions, we need symbols, or variables.  A variable is a type of expression that just contains a value as child, and that has an `assign` method to assign a value to the variable.  The `assign` method can be used to modify the variable's content (without `assign`, our variables would be constants!). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d89564b1fb373ebc6b9367afc6fdc062",
          "grade": false,
          "grade_id": "cell-8a93f8262ec52081",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "j238uBTVPFu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class V(Expr):\n",
        "    \"\"\"This class represents a variable.  The derivative rule corresponds \n",
        "    to d/dx x = 1, but note that it will not be called, since the children\n",
        "    of a variable are just numbers.\"\"\"\n",
        "\n",
        "    def assign(self, v):\n",
        "        \"\"\"Assigns a value to the variable.\"\"\"\n",
        "        self.children = [v]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1712a3ad099adcdb964611efb70854f1",
          "grade": false,
          "grade_id": "cell-9564609a3a3b78ea",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "lUHqo4V-PFvB",
        "colab_type": "text"
      },
      "source": [
        "This suffices for creating expressions.  Let's create one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b34aff49cddb010618ed22f3c6ae4742",
          "grade": false,
          "grade_id": "cell-7247179567bb4c0c",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "4TLVgi_PPFvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed7f9ec5-67aa-4cf0-c139-92972f4d7258"
      },
      "source": [
        "e = V(3) + 4\n",
        "e"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Plus at 0x7f83b60a36d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "51d416786c91b9feb0363f7020046ce7",
          "grade": false,
          "grade_id": "cell-e155e70990661b09",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "AOH-oAS8PFvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us ensure that nose is installed. \n",
        "try:\n",
        "    from nose.tools import assert_equal, assert_true\n",
        "    from nose.tools import assert_false, assert_almost_equal\n",
        "except:\n",
        "    !pip install nose\n",
        "    from nose.tools import assert_equal, assert_true\n",
        "    from nose.tools import assert_false, assert_almost_equal"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "915a644c59bf7bb5bd27aeafd88894a4",
          "grade": false,
          "grade_id": "cell-ad78ae0736decc37",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "M3LuSMjFPFvL",
        "colab_type": "text"
      },
      "source": [
        "## Computing the value of expressions\n",
        "\n",
        "We now have our first expression.  To compute the expression value, we endow each expression with a method `op`, whose task is to compute the value `self.value` of the expression from the list of values `self.values` of the children.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "eee57afb07b73b5aef5fa96287257601",
          "grade": false,
          "grade_id": "cell-401ec9781b95e442",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "YPbbdHk8PFvM",
        "colab_type": "text"
      },
      "source": [
        "Let's implement the `compute` method for an expression.  This method will: \n",
        "\n",
        "1. Loop over the children, and computes the list `self.values` of children values as follows: \n",
        "    * If the child is an expression (an instance of `Expr`, obtain its value by calling `compute` on it. \n",
        "    * If the child is not an instance of `Expr`, then the child must be a number, and we can use its value directly. \n",
        "2. Call the method `op` of the expression, to compute `self.value` from `self.values`. \n",
        "3. return `self.value`. \n",
        "\n",
        "We will let you implement the `compute` method.  Hint: it takes just a couple of lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "f917b165ff042d678e58937865e43a7d",
          "grade": false,
          "grade_id": "cell-cd77d48f0e548dea",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "QUZmb9KzPFvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Expr(object):\n",
        "\n",
        "    def __init__(self, *args):\n",
        "        \"\"\"Initializes an expression node, with a given list of children \n",
        "        expressions.\"\"\"\n",
        "        self.children = args\n",
        "        self.value = None # The value of the expression. \n",
        "        self.values = None # The values of the child expressions.\n",
        "        self.gradient = 0 # The value of the gradient. \n",
        "\n",
        "    def op(self):\n",
        "        \"\"\"This operator must be implemented in subclasses; it should\n",
        "        compute self.value from self.values, thus implementing the \n",
        "        operator at the expression node.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    def compute(self):\n",
        "        \"\"\"This method computes the value of the expression. \n",
        "        It first computes the value of the children expressions,\n",
        "        and then uses self.op to compute the value of the expression.\"\"\"\n",
        "        self.value = None ### INSERT YOUR SOLUTION HERE\n",
        "        return self.value\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\"%s:%r %r (g: %r)\" % (\n",
        "            self.__class__.__name__, self.children, self.value, self.gradient))\n",
        "        \n",
        "    # Expression constructors\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return Plus(self, other)\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        return Plus(self, other)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return Minus(self, other)\n",
        "\n",
        "    def __rsub__(self, other):\n",
        "        return Minus(other, self)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        return Multiply(self, other)\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        return Multiply(other, self)\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        return Divide(self, other)\n",
        "\n",
        "    def __rtruediv__(self, other):\n",
        "        return Divide(other, self)\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        return Power(self, other)\n",
        "\n",
        "    def __rpow__(self, other):\n",
        "        return Power(other, self)\n",
        "\n",
        "    def __neg__(self):\n",
        "        return Negative(self)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "2471180620c22ba253f420e4c732a73c",
          "grade": false,
          "grade_id": "cell-8200e115216d927e",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "sbrLtb59PFvR",
        "colab_type": "text"
      },
      "source": [
        "Let us give `op` for `Plus`, `Multiply`, and for variables via `V`, so you can see how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "80333f72664bc790a0ed03ae1b3b8dd0",
          "grade": false,
          "grade_id": "cell-f499bfe17a2c144",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "qAzjdiiNPFvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class V(Expr):\n",
        "    \"\"\"This class represents a variable.\"\"\"\n",
        "\n",
        "    def assign(self, v):\n",
        "        \"\"\"Assigns a value to the variable.  Used to fit a model, so we\n",
        "        can assign the various input values to the variable.\"\"\"\n",
        "        self.children = [v]\n",
        "\n",
        "    def op(self):\n",
        "        self.value = self.values[0]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Variable: \" + str(self.children[0])\n",
        "\n",
        "        \n",
        "class Plus(Expr):\n",
        "\n",
        "    def op(self):\n",
        "        self.value = self.values[0] + self.values[1]\n",
        "\n",
        "\n",
        "class Multiply(Expr):\n",
        "\n",
        "    def op(self):\n",
        "        self.value = self.values[0] * self.values[1]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "166386bb7c920ddf7a612e68c738cbce",
          "grade": false,
          "grade_id": "cell-5ce618329e626b8c",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "R-t0-A6vPFvX",
        "colab_type": "text"
      },
      "source": [
        "Here you can write your implementation of the `compute` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "8804ad271254cd7eb63c0dd154e9f9d4",
          "grade": false,
          "grade_id": "cell-3504eb6f6aa32994",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "xE2JBEIfPFvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Exercise: Implementation of `compute` method\n",
        "\n",
        "def expr_compute(self):\n",
        "    \"\"\"This method computes the value of the expression. \n",
        "    It first computes the value of the children expressions,\n",
        "    and then uses self.op to compute the value of the expression.\"\"\"\n",
        "    self.values = []  \n",
        "    for x in self.children:\n",
        "      try:\n",
        "        self.values.append(x.compute())\n",
        "      except:\n",
        "        self.values.append(x)\n",
        "    self.op()   \n",
        "    return self.value\n",
        "\n",
        "Expr.compute = expr_compute"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "2d42a74426de42ba788aaac2b99c7796",
          "grade": true,
          "grade_id": "cell-1e0b810fff3932c9",
          "locked": true,
          "points": 10,
          "schema_version": 1,
          "solution": false
        },
        "id": "DHKu0NVJPFvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Tests for compute\n",
        "\n",
        "from nose.tools import assert_equal, assert_true, assert_false\n",
        "\n",
        "# First, an expression consisting only of one variable.\n",
        "e = V(3)\n",
        "assert_equal(e.compute(), 3)\n",
        "assert_equal(e.value, 3)\n",
        "\n",
        "# Then, an expression involving plus.\n",
        "e = V(3) + 4\n",
        "assert_equal(e.compute(), 7)\n",
        "assert_equal(e.value, 7)\n",
        "\n",
        "# And finally, a more complex expression.\n",
        "e = (V(3) + 4) + V(2)\n",
        "assert_equal(e.compute(), 9)\n",
        "assert_equal(e.value, 9)\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "2c609c08ebf4a603fa17aec0a1164285",
          "grade": false,
          "grade_id": "cell-155243fe85cbbc38",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "E0F85ejlPFvg",
        "colab_type": "text"
      },
      "source": [
        "We will have you implement also multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "3398b7fe0c7f7f6b131fe576c1214018",
          "grade": false,
          "grade_id": "cell-e0f70f6e2b1fcaeb",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "Zk3XYfJMPFvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Exercise: Implement `Multiply`\n",
        "\n",
        "class Multiply(Expr):\n",
        "    \"\"\"A multiplication expression.\"\"\"\n",
        "\n",
        "    def op(self):\n",
        "      self.value = 1\n",
        "      for x in self.values:\n",
        "        self.value *=x\n",
        "      return self.value\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "295c3f32c2b48177fe46bb2ab54e3bb1",
          "grade": true,
          "grade_id": "cell-7e141b1f18a436",
          "locked": true,
          "points": 5,
          "schema_version": 1,
          "solution": false
        },
        "id": "wSoAeEpyPFvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Tests for `Multiply`\n",
        "\n",
        "e = V(2) * 3\n",
        "assert_equal(e.compute(), 6)\n",
        "\n",
        "e = (V(2) + 3) * V(4)\n",
        "assert_equal(e.compute(), 20)\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "2560773ffa731d639bf682261ddc3103",
          "grade": false,
          "grade_id": "cell-a5963b5fb573f8ac",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "qkwZcp9yPFvq",
        "colab_type": "text"
      },
      "source": [
        "## Implementing autogradient\n",
        "\n",
        "The next step consists in implementing autogradient.  Consider an expression $e = E(x_0, \\ldots, x_n)$, computed as function of its children expressions $x_0, \\ldots, x_n$.  \n",
        "\n",
        "The goal of the autogradient computation is to accumulate, in each node of the expression, the gradient of the loss with respect to the node's value.  For instance, if the gradient is $2$, we know that if we increase the value of the expression by $\\Delta$, then the value of the loss is increased by $2 \\Delta$. We accumulate the gradient in the field `self.gradient` of the expression. \n",
        "\n",
        "We say _accumulate_ the gradient, because we don't really do:\n",
        "\n",
        "    self.gradient = ...\n",
        "\n",
        "Rather, we have a method `e.zero_gradient()` that sets all gradients to 0, and we then _add_ the gradient to this initial value of 0: \n",
        "\n",
        "    self.gradient += ... \n",
        "\n",
        "We will explain later in detail why we do so; for the moment, just accept it. \n",
        "\n",
        "### Computaton of the gradient\n",
        "\n",
        "In the computation of the autogradient, the expression will receive as input the value $\\partial L / \\partial e$, where $L$ is the loss, and $e$ the value of the expression.  The quantity $\\partial L / \\partial e$ is the gradient of the loss with respect to the expression value. \n",
        "\n",
        "With this input, the method `compute_gradient` of Expr must do the following:\n",
        "\n",
        "* It must _add_ $\\partial L / \\partial e$ to the gradient `self.gradient` of the expression. \n",
        "* It must compute for each child $x_i$ the partial derivative $\\partial e / \\partial x_i$, via a call to the method `derivate`.  The method `derivate` is implemented not for `Expr`, but for each specific operator, such as `Plus`, `Multiply`, etc: each operator knows how to compute the derivative with respect to its arguments.\n",
        "* It must propagate to each child $x_i$ the gradient $\\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial x_i}$, by calling the method `compute_gradient` of the child with argument $\\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial x_i}$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "792950d81591dc07614a72eb4fa6202a",
          "grade": false,
          "grade_id": "cell-48a57596316cd692",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "7Q0dbldePFvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expr_derivate(self):\n",
        "    \"\"\"This method computes the derivative of the operator at the expression\n",
        "    node.  It needs to be implemented in derived classes, such as Plus, \n",
        "    Multiply, etc.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "Expr.derivate = expr_derivate\n",
        "\n",
        "def expr_zero_gradient(self):\n",
        "    \"\"\"Sets the gradient to 0, recursively for this expression\n",
        "    and all its children.\"\"\"\n",
        "    self.gradient = 0\n",
        "    for e in self.children:\n",
        "        if isinstance(e, Expr):\n",
        "            e.zero_gradient()\n",
        "\n",
        "Expr.zero_gradient = expr_zero_gradient\n",
        "\n",
        "def expr_compute_gradient(self, de_loss_over_de_e=1):\n",
        "    \"\"\"Computes the gradient.\n",
        "    de_loss_over_de_e is the gradient of the output. \n",
        "    de_loss_over_de_e will be added to the gradient, and then \n",
        "    we call for each child the method compute_gradient, \n",
        "    with argument de_loss_over_de_e * d expression / d child.\n",
        "    The value d expression / d child is computed by self.derivate. \"\"\"\n",
        "    pass ### PLACEHOLDER FOR YOUR SOLUTION.\n",
        "                \n",
        "Expr.compute_gradient = expr_compute_gradient"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "40e323b7db91da2f227c678c47f21a3d",
          "grade": false,
          "grade_id": "cell-19c311688ac25ce1",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "0QlaV0HaPFvu",
        "colab_type": "text"
      },
      "source": [
        "Let us endow our operators `V`, `Plus`, `Multiply` with the `derivate` method, so you can see how it works in practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d87167fb2f4e36a4c5c2e3d335c19601",
          "grade": false,
          "grade_id": "cell-d93a5e0205c20442",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "VaWEsOIzPFvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class V(Expr):\n",
        "    \"\"\"This class represents a variable.  The derivative rule corresponds \n",
        "    to d/dx x = 1, but note that it will not be called, since the children\n",
        "    of a variable are just numbers.\"\"\"\n",
        "\n",
        "    def assign(self, v):\n",
        "        \"\"\"Assigns a value to the variable.  Used to fit a model, so we\n",
        "        can assign the various input values to the variable.\"\"\"\n",
        "        self.children = [v]\n",
        "\n",
        "    def op(self):\n",
        "        self.value = self.values[0]\n",
        "\n",
        "    def derivate(self):\n",
        "        return [1.] # This is not really used.\n",
        "\n",
        "        \n",
        "class Plus(Expr):\n",
        "    \"\"\"An addition expression.  The derivative rule corresponds to \n",
        "    d/dx (x+y) = 1, d/dy (x+y) = 1\"\"\"\n",
        "\n",
        "    def op(self):\n",
        "        self.value = self.values[0] + self.values[1]\n",
        "\n",
        "    def derivate(self):\n",
        "        return [1., 1.]\n",
        "\n",
        "\n",
        "class Multiply(Expr):\n",
        "    \"\"\"A multiplication expression. The derivative rule corresponds to\n",
        "    d/dx (xy) = y, d/dy(xy) = x\"\"\"\n",
        "\n",
        "    def op(self):\n",
        "        self.value = self.values[0] * self.values[1]\n",
        "\n",
        "    def derivate(self):\n",
        "        return [self.values[1], self.values[0]]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "6d93ce76873aa04d1f7df51b42716e1d",
          "grade": false,
          "grade_id": "cell-890e2f2a23220658",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "JG4o5XLePFvy",
        "colab_type": "text"
      },
      "source": [
        "Let us comment on some subtle points, before you get to work at implementing `compute_gradient`.  \n",
        "\n",
        "**`zero_gradient`:** First, notice how in the implementation of `zero_gradient`, when we loop over the children, we check whether each children is an `Expr` via isinstance(e, Expr).  In general, we have to remember that children can be either `Expr`, or simply numbers, and of course numbers do not have methods such as `zero_gradient` or `compute_gradient` implemented for them. \n",
        "\n",
        "**`derivate`:** Second, notice how `derivate` is not implemented in `Expr` directly, but rather, only in the derived classes such as `Plus`.  The derivative of the expression with respect to its arguments depends on which function it is, obviously.  \n",
        "\n",
        "For `Plus`, we have $e = x_0 + x_1$, and so: \n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial x_0} = 1 \\qquad \\frac{\\partial e}{\\partial x_1} = 1 \\; ,\n",
        "$$\n",
        "\n",
        "because $d(x+y)/dx = 1$.  Hence, the `derivate` method of `Plus` returns\n",
        "\n",
        "$$\n",
        "\\left[ \\frac{\\partial e}{\\partial x_0}, \\: \\frac{\\partial e}{\\partial x_1}\\right] \\; = \\; [1, 1] \\; .\n",
        "$$\n",
        "\n",
        "For `Multiply`, we have $e = x_0 \\cdot x_1$, and so:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial x_0} = x_1 \\qquad \\frac{\\partial e}{\\partial x_1} = x_0 \\; ,\n",
        "$$\n",
        "\n",
        "because $d(xy)/dx = y$.  Hence, the `derivate` method of `Plus` returns\n",
        "\n",
        "$$\n",
        "\\left[ \\frac{\\partial e}{\\partial x_0}, \\: \\frac{\\partial e}{\\partial x_1}\\right] \\; = \\; [x_1, x_0] \\; .\n",
        "$$\n",
        "\n",
        "\n",
        "**Calling `compute` before `compute_gradient`:** Lastly, a very important point: when calling `compute_gradient`, we will assume that `compute` has _already_ been called.  In this way, the value of the expression, and its children, are available for the computation of the gradient.  Note how in `Multiply.derivate` we use these values in order to compute the partial derivatives. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "2038156da2d8cd61443e53f596d781c9",
          "grade": false,
          "grade_id": "cell-df110b7e18b9a664",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "njfRCvZfPFvz",
        "colab_type": "text"
      },
      "source": [
        "With these clarifications, we ask you to implement the `compute_gradient` method, which again must:\n",
        "\n",
        "* _add_ $\\partial L / \\partial e$ to the gradient `self.gradient` of the expression; \n",
        "* compute $\\frac{\\partial e}{\\partial x_i}$ for each child $x_i$ by calling the method `derivate` of itself; \n",
        "* propagate to each child $x_i$ the gradient $\\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial x_i}$, by calling the method `compute_gradient` of the child with argument $\\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial x_i}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "2d44dd162fa977588801ebe22596555e",
          "grade": false,
          "grade_id": "cell-406125234a6b7ace",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "UmNowWogPFv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Exercise: Implementation of `compute_gradient`\n",
        "\n",
        "def expr_compute_gradient(self, de_loss_over_de_e=1):\n",
        "    \"\"\"Computes the gradient.\n",
        "    de_loss_over_de_e is the gradient of the output. \n",
        "    de_loss_over_de_e will be added to the gradient, and then \n",
        "    we call for each child the method compute_gradient, \n",
        "    with argument de_loss_over_de_e * d expression / d child.\n",
        "    The value d expression / d child is computed by self.derivate. \"\"\"\n",
        "    self.gradient += de_loss_over_de_e\n",
        "    i = 0\n",
        "    for x in self.children:\n",
        "      if isinstance(x, Expr):\n",
        "        x.compute_gradient(de_loss_over_de_e * self.derivate()[i])\n",
        "      i+=1\n",
        "\n",
        "      \n",
        "        \n",
        "                \n",
        "Expr.compute_gradient = expr_compute_gradient"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "c29a9acddd0e19fdff2586c5ae6921a1",
          "grade": true,
          "grade_id": "cell-d8a6b02923b10cfb",
          "locked": true,
          "points": 15,
          "schema_version": 1,
          "solution": false
        },
        "id": "9YOpajZyPFv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Tests for `compute_gradient`\n",
        "\n",
        "# First, the gradient of a sum.\n",
        "vx = V(3)\n",
        "vz = V(4)\n",
        "y = vx + vz\n",
        "assert_equal(y.compute(), 7)\n",
        "y.zero_gradient()\n",
        "y.compute_gradient()\n",
        "assert_equal(vx.gradient, 1.)\n",
        "\n",
        "# Second, the gradient of a product.\n",
        "vx = V(3)\n",
        "vz = V(4)\n",
        "y = vx * vz\n",
        "assert_equal(y.compute(), 12)\n",
        "y.zero_gradient()\n",
        "y.compute_gradient()\n",
        "assert_equal(vx.gradient, 4)\n",
        "assert_equal(vz.gradient, 3)\n",
        "\n",
        "# Finally, the gradient of the product of sums. \n",
        "\n",
        "vx = V(1)\n",
        "vw = V(3)\n",
        "vz = V(4)\n",
        "y = (vx + vw) * (vz + 3)\n",
        "assert_equal(y.compute(), 28)\n",
        "y.zero_gradient()\n",
        "y.compute_gradient()\n",
        "assert_equal(vx.gradient, 7)\n",
        "assert_equal(vz.gradient, 4)\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "a7e60a89686ed6d2532e26f9c4c77c16",
          "grade": false,
          "grade_id": "cell-270cbd6ceefc8cac",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "1A8kyXzRPFv6",
        "colab_type": "text"
      },
      "source": [
        "## Why do we accumulate gradients?\n",
        "\n",
        "We are now in the position of answering the question of why we accumulate gradients.  There are two reasons. \n",
        "\n",
        "### Multiple variable occurrence\n",
        "\n",
        "The most important reason why we need to _add_ to the gradient of each node is that nodes, and in particular, variable nodes, can occur in multiple places in an expression tree.  To compute the total influence of the variable on the expression, we need to _sum_ the influence of each occurrence.  Let's see this with a simple example.  Consider the expression $y = x \\cdot x$.  We can code it as follows: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1b79ce686ed71ae08e4d3d458428b6cf",
          "grade": false,
          "grade_id": "cell-fdb44466b16f032c",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "81OXmnbrPFv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vx = V(2.) # Creates a variable vx and initializes it to 2.\n",
        "y = vx * vx"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d47e6e1af7e3d461854ac57db7b49e40",
          "grade": false,
          "grade_id": "cell-786a51429b1522f7",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "HtNyhFjwPFwA",
        "colab_type": "text"
      },
      "source": [
        "For $y = x^2$, we have $dy / dx = 2x = 4$, given that $x=2$.  How is this reflected in our code? \n",
        "\n",
        "Our code considers separately the left and right occurrences of `vx` in the expression; let us denote them with $vx_l$ and $vx_r$.  The expression can be written as $y = vx_l \\cdot vx_r$, and we have that $\\partial y / \\partial \\, vx_l = vx_r = 2$, as $vx_r = 2$.  Similarly, $\\partial y / \\partial \\, vx_r = 2$.  These two gradients are added to `vx.gradient` by the method `compute_gradient`, and we get that the total gradient is 4, as desired."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "7f4a64ca18bca654a54d7f2b7c6aaf38",
          "grade": false,
          "grade_id": "cell-15a4fe7918211c0f",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "vvnoYKnyPFwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b41d6184-4a5f-40cc-e996-8a1ad7ea5f12"
      },
      "source": [
        "y.compute() # We have to call compute() before compute_gradient()\n",
        "y.zero_gradient()\n",
        "y.compute_gradient()\n",
        "print(\"gradient of vx:\", vx.gradient)\n",
        "assert_equal(vx.gradient, 4)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradient of vx: 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "0f5aca17a4b71679f3fd22b6c4720852",
          "grade": false,
          "grade_id": "cell-381479a2d2ea9329",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "wAZqqyJrPFwD",
        "colab_type": "text"
      },
      "source": [
        "### Multiple data to fit\n",
        "\n",
        "The other reason why we need to tally up the gradient is that in general, we need to fit a function to more than one data point.  Assume that we are given a set of inputs $x_1, x_2, \\ldots, x_n$ and desired outputs $y_1, y_2, \\ldots, y_n$. Our goal is to approximate the desired ouputs via an expression $e(x, \\theta)$ of the input, according to some parameters $\\theta$.  Our goal is to choose the parameters $\\theta$ to minimize the sum of the square errors for the points: \n",
        "\n",
        "$$\n",
        "L_{tot}(\\theta) = \\sum_{i=1}^n L_i(\\theta) \\; ,\n",
        "$$\n",
        "\n",
        "where $L_i(\\theta) = (e(x_i, \\theta) - y_i)^2$ is the loss for a single data point.  The gradient $L_{tot}(\\theta)$ with respect to $\\theta$ can be computed by adding up the gradients for the individual points:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\theta} L_{tot}(\\theta) \\;=\\;\n",
        "\\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta} L_i(\\theta) \\; .\n",
        "$$\n",
        "\n",
        "To translate this into code, we will build an expression $e(x, \\theta)$ involving the input $x$ and the parameters $\\theta$, and an expression \n",
        "\n",
        "$$\n",
        "L = (e(x, \\theta) - y)^2\n",
        "$$\n",
        "\n",
        "for the loss, involving $x, y$ and $\\theta$.  We will then zero all gradients via `zero_gradient`. Once this is done, we compute the loss $L$ for each point, and then the gradient $\\partial L / \\partial \\theta$ via a call to `compute_gradient`.  The gradients for all the points will be added, yielding the gradient for minimizing the total loss. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b4c34a5fe1f4c06ff12e2f9152eecd05",
          "grade": false,
          "grade_id": "cell-2bc7acb3a197711a",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "XAsKhQd6PFwE",
        "colab_type": "text"
      },
      "source": [
        "## Rounding up the implementation\n",
        "\n",
        "Now that we have implemented autogradient, as well as the operators `Plus` and `Multiply`, it is time to implement the remaining operators: \n",
        "\n",
        "* `Minus`\n",
        "* `Divide` (no need to worry about division by zero)\n",
        "* `Power`, representing exponentiation (the `**` operator of Python)\n",
        "* and the unary minus `Negative`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "b77a1f3a9b9575c78a872c43ee62c277",
          "grade": false,
          "grade_id": "cell-529921db43de3baa",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "eap3Io0rPFwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Exercise: Implementation of `Minus`, `Divide`, `Power`, and `Negative`\n",
        "\n",
        "import math\n",
        "\n",
        "class Minus(Expr):\n",
        "    \"\"\"Operator for x - y\"\"\"\n",
        "    def op(self):\n",
        "      self.value = self.values[0] - self.values[1]\n",
        "\n",
        "\n",
        "    def derivate(self):\n",
        "        return [1., -1.]\n",
        "\n",
        "class Divide(Expr):\n",
        "    \"\"\"Operator for x / y\"\"\"\n",
        "\n",
        "    def op(self):\n",
        "      self.value = self.values[0] / self.values[1]\n",
        "\n",
        "    def derivate(self):\n",
        "      return [1/self.values[1], -self.values[0]/(self.values[1]**2)]\n",
        "\n",
        "class Power(Expr):\n",
        "    \"\"\"Operator for x ** y\"\"\"  \n",
        "    def op(self):\n",
        "      self.value = self.values[0] ** self.values[1]\n",
        "\n",
        "    def derivate(self):\n",
        "        return [self.values[1]*(self.values[0]**(self.values[1]-1)),math.log(self.values[0])*(self.values[0]**self.values[1])]\n",
        "\n",
        "class Negative(Expr):\n",
        "    \"\"\"Operator for -x\"\"\"\n",
        "\n",
        "    def op(self):\n",
        "      self.value = -1*self.values[0] \n",
        "\n",
        "    def derivate(self):\n",
        "        return [-1]\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "ae67f4327dc668fcecb93c60068d9591",
          "grade": false,
          "grade_id": "cell-fc55e594a0f75d32",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "8m7PWquGPFwI",
        "colab_type": "text"
      },
      "source": [
        "Here are some tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "5694cc71dd23d28a2f507a92e45fa4de",
          "grade": true,
          "grade_id": "cell-2f410d187bb3953b",
          "locked": true,
          "points": 10,
          "schema_version": 1,
          "solution": false
        },
        "id": "7WiKkVJbPFwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Tests for `Minus`\n",
        "\n",
        "# Minus. \n",
        "vx = V(3)\n",
        "vy = V(2)\n",
        "e = vx - vy\n",
        "assert_equal(e.compute(), 1.)\n",
        "e.zero_gradient()\n",
        "e.compute_gradient()\n",
        "assert_equal(vx.gradient, 1)\n",
        "assert_equal(vy.gradient, -1)\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "89fec5f8ce2a86fbaf976fd08f0359f9",
          "grade": true,
          "grade_id": "cell-4b2aa7944b918b42",
          "locked": true,
          "points": 10,
          "schema_version": 1,
          "solution": false
        },
        "id": "rIOrnihPPFwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Tests for `Divide`\n",
        "\n",
        "from nose.tools import assert_almost_equal\n",
        "\n",
        "# Divide. \n",
        "vx = V(6)\n",
        "vy = V(2)\n",
        "e = vx / vy\n",
        "assert_equal(e.compute(), 3.)\n",
        "e.zero_gradient()\n",
        "e.compute_gradient()\n",
        "assert_equal(vx.gradient, 0.5)\n",
        "assert_equal(vy.gradient, -1.5)\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d23a960895718c2021b52cd9342bf307",
          "grade": true,
          "grade_id": "cell-27cfe444d23610f0",
          "locked": true,
          "points": 10,
          "schema_version": 1,
          "solution": false
        },
        "id": "uU6wcpGGPFwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Tests for `Power`\n",
        "\n",
        "from nose.tools import assert_almost_equal\n",
        "\n",
        "# Power. \n",
        "vx = V(2)\n",
        "vy = V(3)\n",
        "e = vx ** vy\n",
        "assert_equal(e.compute(), 8.)\n",
        "e.zero_gradient()\n",
        "e.compute_gradient()\n",
        "assert_equal(vx.gradient, 12.)\n",
        "assert_almost_equal(vy.gradient, math.log(2.) * 8., places=4)\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "9907f2ee5d7b81aa913fc33e24ba46ab",
          "grade": true,
          "grade_id": "cell-b83a719bc4cb9a65",
          "locked": true,
          "points": 10,
          "schema_version": 1,
          "solution": false
        },
        "id": "Foq_JT3uPFwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Tests for `Negative`\n",
        "\n",
        "from nose.tools import assert_almost_equal\n",
        "\n",
        "# Negative\n",
        "vx = V(6)\n",
        "e = - vx\n",
        "assert_equal(e.compute(), -6.)\n",
        "e.zero_gradient()\n",
        "e.compute_gradient()\n",
        "assert_equal(vx.gradient, -1.)\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "38a6038dc933fcb0fbfe66ef8c25df2b",
          "grade": false,
          "grade_id": "cell-ad0d23f9d8e69b76",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "-MGAue3rPFwV",
        "colab_type": "text"
      },
      "source": [
        "## Optimization\n",
        "\n",
        "Let us use our ML framework to fit a parabola to a given set of points.  Here is our set of points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "66bfb074f4cc0785a76190432fb07fe4",
          "grade": false,
          "grade_id": "cell-b719e13a8db7194b",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "uAfpRrFMPFwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "points = [\n",
        "    (-2, 2.7),\n",
        "    (-1, 3),\n",
        "    (0, 1.3),\n",
        "    (1, 2.4),\n",
        "    (3, 5.5),\n",
        "    (4, 6.2),\n",
        "    (5, 9.1),\n",
        "]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "038a600d6650b27f26125e805ec69e1f",
          "grade": false,
          "grade_id": "cell-254e3bee4ee59156",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "DHxg9NwgPFwY",
        "colab_type": "text"
      },
      "source": [
        "Let us display these points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "9291f9d742df59773bcadf489e32dc9e",
          "grade": false,
          "grade_id": "cell-54fd8c743b451183",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "IQO5yZSDPFwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = (8.0, 3.)\n",
        "params = {'legend.fontsize': 'large',\n",
        "          'axes.labelsize': 'large',\n",
        "          'axes.titlesize':'large',\n",
        "          'xtick.labelsize':'large',\n",
        "          'ytick.labelsize':'large'}\n",
        "matplotlib.rcParams.update(params)\n",
        "\n",
        "def plot_points(points):\n",
        "    fig, ax = plt.subplots()\n",
        "    xs, ys = zip(*points)\n",
        "    ax.plot(xs, ys, 'r+')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.show()"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "5b1424e5df72c4a534ac7ebc828c44a1",
          "grade": false,
          "grade_id": "cell-c454611b4ca61393",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "zlHMIIF-PFwa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "06481730-753d-4b25-fa33-09f43ce510b4"
      },
      "source": [
        "plot_points(points)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAADWCAYAAAATvLEfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANJUlEQVR4nO3db4xldX3H8feHHaKW7Ypmx41iylgDDdqmK94mTRoKD2gQktambSqW/nnQFgPBphATfUCzs2BjTExtIojZdqtETBtikFDqn6TGxmii9W6CbYmU2iC6LRsHRWGBXal+++AO7Didhdntvefc37nvV3LCnXsu8/2e3Dn57O93/qWqkCRJ8+2MvhuQJEkvzMCWJKkBBrYkSQ0wsCVJaoCBLUlSAwxsSZIasNR3A89n9+7dtbKy0ncbkiR14tChQ49W1fJW6+Y6sFdWVhiPx323IUlSJ5I8fLJ1TolLktQAA1uSpAYY2JIkna7V1c5KGdiSJJ2u/fs7K2VgS5LUAANbkqRTsboKyWSBE69nPD2eeX685mg0Ki/rkiTNrQSmmKNJDlXVaKt1jrAlSWqAgS1J0unat6+zUga2JEmny8u6JEnSRga2JEkNMLAlSWqAgS1JUgMMbEmSGmBgS5LUAANbkqQGGNiSJDXAwJYkqQEGtiRJDTCwJUlqgIEtSVIDDGxJkhrQWWAnWUnyySSPJTmS5JYkS13VlySpZV2OsD8IfBt4JbAXuBi4tsP6kiQ1q8vAfg1wZ1Udq6ojwKeB13dYX5KkZnUZ2H8JXJnkJ5KcA1zOJLR/TJKrk4yTjNfW1jpsT5Kk+dVlYH+eyYj6ceAwMAbu3vyhqjpQVaOqGi0vL3fYniRJ86uTwE5yBpPR9F3AWcBu4GXAe7uoL0lS67oaYb8c+Cnglqo6XlXfAT4MXNFRfUmSmtZJYFfVo8BDwDVJlpKcDfwB8C9d1JckqXVdHsP+DeBNwBrwdeAZ4PoO60uS1KzOblxSVfcBl3RVT5KkIfHWpJIkNcDAliSpAQa2JEkNMLAlSWqAgS1JUgMMbEmSGmBgS5LUAANbkqQGGNiSJDXAwJYkqQEGtiRJDTCwJUlqgIEtSVIDDGxJkhpgYEuS1AADW5KkBhjYkiQ1wMCWJKkBBrYkSQ0wsCVJaoCBLUlSAwxsSZIaYGBLktQAA1uSpAYY2JKk6Vtd7buDwTGwJUnTt39/3x0MjoEtSVIDDGxJ0nSsrkIyWeDEa6fHp6LTwE5yZZKvJXkyyX8muajL+pKkGVpdharJAideG9hTsdRVoSS/ArwXeAvwz8Aru6otSVLrOgtsYD9wU1V9af3n/+qwtiSpS/v29d3B4HQyJZ5kBzAClpN8PcnhJLckeckWn706yTjJeG1trYv2JEnT5jT41HV1DHsPcCbwW8BFwF7gDcCNmz9YVQeqalRVo+Xl5Y7akyRpvnUV2E+v//cDVfVIVT0K/AVwRUf1JUlqWieBXVWPAYeB2vh2F7UlSRqCLi/r+jDw9iSvSPIy4Hrg3g7rS5LUrC7PEr8Z2A08CBwD7gT+vMP6kiQ1q7PArqpngGvXF0mSdAq8NakkdcnLnXSaDGxJ6pJPsdJpMrAlSWqAgS1Js+ZTrDQFqZrfy6FHo1GNx+O+25Ck6UlOPM1K2iTJoaoabbXOEbYkSQ3YdmAneX+SvbNsRpIGz6dY6TSdygh7B/CZJP+W5J1JXj2rpiRpsDxurdO07cCuqj8BXgW8i8nTtr6W5B+T/H6SnbNqUJIkneIx7Kr6YVXdW1VvBX4RWAY+AhxJ8tdJzplBj5IkLbxTCuwku5L8YZLPAZ8Hvszk+dYXAEeBT02/RUmStO17iSf5OHAZk6D+EHB3VR3fsP4G4PtT71CSJJ3Swz++BFxXVUe2WllVP0qyZzptSZKkjbYd2FX1vm185qn/XzuSJGkr3jhFkqQGGNiSJDXAwJYkqQEGtiRJDTCwJUlqgIEtSVIDDGxJkhpgYEuS1AADW5KkBhjYkiQ1wMCWJKkBBrYkSQ0wsCVJaoCBLUlSAzoP7CTnJTmW5I6ua0uS1Ko+Rti3Al/poa4kSc3qNLCTXAl8D/hsl3UlSWpdZ4GdZBdwE3DDC3zu6iTjJOO1tbVumpMkac51OcK+GThYVYef70NVdaCqRlU1Wl5e7qg1SZLm21IXRZLsBS4F3tBFPUmShqaTwAYuAVaAbyYB2AnsSPK6qrqwox4kSWpWV4F9APi7DT+/g0mAX9NRfUmSmtZJYFfVU8BTz/6c5ChwrKo8q0ySpG3oaoT9Y6pqtY+6kiS1yluTSpLUAANbkqQGGNiSJDXAwJYkqQEGtiRJDTCwJUlqgIEtSVIDDGxJkhpgYEuS1AADe4hWV/vuQJI0ZQb2EO3f33cHkqQpM7AlSWqAgT0Uq6uQTBY48drpcUkahFRV3z2c1Gg0qvF43Hcb7Ulgjr9XSdLWkhyqqtFW6xxhS5LUgMUK7EWZHt63r+8OJElTtlhT4k4VS5LmmFPikiQ1bviB7dnTkqQBcEpckqQ54ZS41DJngySxaIHt2dNqkbealcSiBbYjFUlSoxYrsKVWeLKkpE0W66QzqUWeLCktDE86kySpcQa2NO88WVISBrY0/zxuLYmOAjvJi5IcTPJwkieS3Jfk8i5qS5I0BF2NsJeAbwEXAy8FbgTuTLLSUX1Jkpq21EWRqnoSWN3w1r1JHgLeCHyjix4kSWpZL8ewk+wBzgfu32Ld1UnGScZra2vdNydJ0hzqPLCTnAl8DLi9qh7YvL6qDlTVqKpGy8vLXbcnSdJc6jSwk5wBfBT4AXBdl7UlSWpZJ8ewAZIEOAjsAa6oqme6qi1JUus6C2zgNuAC4NKqerrDupIkNa+r67DPBd4G7AWOJDm6vlzVRX1JklrX1WVdDwPpopYkSUPkrUklSWqAgS1JUgMMbEmSGmBgq10+xUrSAjGw1a79+/vuQJI6Y2BLktQAA1ttWV2FZLLAiddOj0sauFRV3z2c1Gg0qvF43HcbmlcJzPHfrySdqiSHqmq01TpH2JIkNcDAVrv27eu7A0nqjIGtdnncWtICMbAlSWqAgS1JUgMMbEmSGjDXl3UlWQMenuKv3A08OsXfN6/czmFxO4fF7RyWaW/nuVW1vNWKuQ7saUsyPtn1bUPidg6L2zksbuewdLmdTolLktQAA1uSpAYsWmAf6LuBjridw+J2DovbOSydbedCHcOWJKlVizbCliSpSQa2JEkNWLjATvKiJAeTPJzkiST3Jbm8775mIcl1ScZJjif5SN/9TEuSlyf5RJIn17/H3+m7p1kY6ve30YLtj3ckeSTJ40keTPJHffc0S0nOS3IsyR199zILSf5pffuOri//PuuaCxfYwBLwLeBi4KXAjcCdSVZ67GlW/ht4N/A3fTcyZbcCPwD2AFcBtyV5fb8tzcRQv7+NFml/fA+wUlW7gF8D3p3kjT33NEu3Al/pu4kZu66qdq4vPzPrYgsX2FX1ZFWtVtU3qupHVXUv8BAwuB2nqu6qqruB7/Tdy7QkOQv4TeDPqupoVX0BuAf4vX47m74hfn+bLdj+eH9VHX/2x/XltT22NDNJrgS+B3y2716GZOECe7Mke4Dzgfv77kXbcj7wP1X14Ib3vgoMcYS9cIa+Pyb5YJKngAeAR4BP9tzS1CXZBdwE3NB3Lx14T5JHk3wxySWzLrbQgZ3kTOBjwO1V9UDf/WhbdgKPb3rv+8BP9tCLpmgR9sequpbJ3+pFwF3A8ef/P5p0M3Cwqg733ciMvRP4aeAcJtdi/32Smc6YDC6w108EqJMsX9jwuTOAjzI5Fnpdbw2fpu1u5wAdBXZtem8X8EQPvWhKWt8fT0VV/XD9UM6rgWv67meakuwFLgXe33cvs1ZVX66qJ6rqeFXdDnwRuGKWNZdm+cv7UFWXvNBnkgQ4yOSkpSuq6plZ9zVt29nOgXoQWEpyXlX9x/p7P89Ap1AXwRD2x9O0xPCOYV8CrADfnHyt7AR2JHldVV3YY19dKCCzLDC4EfY23QZcAPxqVT3ddzOzkmQpyYuBHUx2mhcnafofaVX1JJOpxJuSnJXkl4A3MxmdDcoQv7+TGPz+mOQVSa5MsjPJjiSXAW9leCdlHWDyj5C968uHgH8ALuuzqWlLcnaSy57dJ5NcBfwy8OlZ1l24wE5yLvA2Jn9MRzZcQ3dVz63Nwo3A08C7gN9df31jrx1Nx7XAS4BvA38LXFNVQxxhD/X7e84C7Y/FZPr7MPAY8D7gT6vqnl67mrKqeqqqjjy7MDmEdayq1vrubcrOZHLJ5RqTZ2G/Hfj1TSfDTp33EpckqQELN8KWJKlFBrYkSQ0wsCVJaoCBLUlSAwxsSZIaYGBLktQAA1uSpAYY2JIkNcDAliSpAQa2JACSvDbJd5NcuP7zq5KsdfGcX0kvzFuTSnpOkj8GrgdGwCeAf62qd/TblSQwsCVtkuQe4DVMHljxC1V1vOeWJOGUuKT/66+AnwU+YFhL88MRtqTnJNkJfBX4HHA58HNV9d1+u5IEBrakDZIcBHZW1VuSHADOrqrf7rsvSU6JS1qX5M3Am4Br1t+6AbgwyVX9dSXpWY6wJUlqgCNsSZIaYGBLktQAA1uSpAYY2JIkNcDAliSpAQa2JEkNMLAlSWqAgS1JUgMMbEmSGvC/6lNqKQtODBAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "99b73a094c69f1cdef90db7538253b96",
          "grade": false,
          "grade_id": "cell-d680895711b1ff83",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "g73kSJg4PFwd",
        "colab_type": "text"
      },
      "source": [
        "To fit a parabola to these points, we will build an `Expr` that represents the equation $\\hat{y} = ax^2 + bx + c$, where $\\hat{y}$ is the value of $y$ predicted by our parabola. \n",
        "If $\\hat{y}$ is the predicted value, and $y$ is the observed value, to obtain a better prediction of the observations, we minimize the loss $L = (\\hat{y} - y)^2$, that is, the square prediction error. \n",
        "Written out in detail, our loss is:\n",
        "\n",
        "$$\n",
        "    L \\;=\\; \\left( y - \\hat{y}\\right)^ 2 \\;=\\; \\left( y - (ax^2 + bx + c) \\right)^2 \\; .\n",
        "$$\n",
        "\n",
        "Here, $a, b, c$ are parameters that we need to tune to minimize the loss, and obtain a good fit between the parabola and the points. \n",
        "This tuning, or training, is done by repeating the following process many times:\n",
        "\n",
        "* Zero the gradient\n",
        "* For each point:\n",
        "    * Set the values of x, y to the value of the point.\n",
        "    * Compute the expression giving the loss.\n",
        "    * Backpropagate.  This computes all gradients with respect to the loss, and in particular, the gradients of the coefficients $a, b, c$. \n",
        "* Update the coefficients $a, b, c$ by taking a small step in the direction of the negative gradient (negative, so that the loss decreases)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "686883a48bd394b9f6a4c3823488e7ec",
          "grade": false,
          "grade_id": "cell-f88be5a26cd12a63",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "PMvzVWqRPFwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "va = V(0.)\n",
        "vb = V(0.)\n",
        "vc = V(0.)\n",
        "vx = V(0.)\n",
        "vy = V(0.)\n",
        "\n",
        "oy = va * vx * vx + vb * vx + vc\n",
        "\n",
        "loss = (vy - oy) * (vy - oy)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "e6f7f5f6eac58838ba757b327ea3e0a0",
          "grade": false,
          "grade_id": "cell-c2b0da3544d9add8",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "r3v8vtmzPFwf",
        "colab_type": "text"
      },
      "source": [
        "Below, implement the \"for each point\" part of the above informal description.  Hint: this takes about 4-5 lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "76387e541752e2ac1852e6eb7d0384c8",
          "grade": false,
          "grade_id": "cell-cf54345d957356",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "viOhnESPPFwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(loss, points, params, delta=0.0001, num_iterations=4000):\n",
        "\n",
        "    for iteration_idx in range(num_iterations):\n",
        "        loss.zero_gradient()\n",
        "        total_loss = 0.\n",
        "        for x, y in points:\n",
        "            ### You need to implement here the computaton of the \n",
        "            ### loss gradient for the point (x, y). \n",
        "            total_loss += loss.value\n",
        "        if (iteration_idx + 1) % 100 == 0:\n",
        "            print(\"Loss:\", total_loss)\n",
        "        for vv in params:\n",
        "            vv.assign(vv.value - delta * vv.gradient)\n",
        "    return total_loss"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "380c6c0a8b02b21489c8f4d0e7a3731d",
          "grade": false,
          "grade_id": "cell-4b2a5a0e2152c1d1",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "4P-0brqUPFwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Exercise: Implementation of `fit`\n",
        "\n",
        "def fit(loss, points, params, delta=0.0001, num_iterations=4000):\n",
        "\n",
        "    for iteration_idx in range(num_iterations):\n",
        "        loss.zero_gradient()\n",
        "        total_loss = 0.\n",
        "        for x, y in points:\n",
        "          vx.assign(x)\n",
        "          vy.assign(y)\n",
        "          loss.compute()\n",
        "          loss.compute_gradient()\n",
        "          total_loss += loss.value\n",
        "        if (iteration_idx + 1) % 100 == 0:\n",
        "            print(\"Loss:\", total_loss)\n",
        "        for vv in params:\n",
        "            vv.assign(vv.value - delta * vv.gradient)\n",
        "    return total_loss"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "41f2dc0de7b40868d3ee87d0ee707f73",
          "grade": false,
          "grade_id": "cell-39a398ffbc71ec70",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "LFjIJ0ZiPFwj",
        "colab_type": "text"
      },
      "source": [
        "Let's train the coefficients `va`, `vb`, `vc`: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "733adca5440c5e9e48deb92751eeb84b",
          "grade": false,
          "grade_id": "cell-aa7a64de5eb95b8c",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "6HL5_P21PFwk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "99c9f429-84fb-43af-b772-4e98a7f99733"
      },
      "source": [
        "from nose.tools import assert_less\n",
        "\n",
        "lv = fit(loss, points, [va, vb, vc])\n",
        "assert_less(lv, 2.5)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 15.691480263172831\n",
            "Loss: 13.628854145973717\n",
            "Loss: 11.95710461470721\n",
            "Loss: 10.577288435238483\n",
            "Loss: 9.421709915417068\n",
            "Loss: 8.442894724380599\n",
            "Loss: 7.606627717852742\n",
            "Loss: 6.887533665385938\n",
            "Loss: 6.266252079473012\n",
            "Loss: 5.727613949776414\n",
            "Loss: 5.259450432876111\n",
            "Loss: 4.851802101057622\n",
            "Loss: 4.4963837663728965\n",
            "Loss: 4.18621382152429\n",
            "Loss: 3.9153507183966223\n",
            "Loss: 3.6787002636071753\n",
            "Loss: 3.471870598609928\n",
            "Loss: 3.2910600093895632\n",
            "Loss: 3.13296792199636\n",
            "Loss: 2.9947227347465053\n",
            "Loss: 2.8738222326925422\n",
            "Loss: 2.768083672161589\n",
            "Loss: 2.6756014919475786\n",
            "Loss: 2.594711177586694\n",
            "Loss: 2.523958185206276\n",
            "Loss: 2.462071090153853\n",
            "Loss: 2.4079383059994615\n",
            "Loss: 2.360587848696134\n",
            "Loss: 2.3191697158813342\n",
            "Loss: 2.28294052348259\n",
            "Loss: 2.251250098021605\n",
            "Loss: 2.2235297678961032\n",
            "Loss: 2.199282133501638\n",
            "Loss: 2.1780721263873506\n",
            "Loss: 2.1595191931327227\n",
            "Loss: 2.1432904612844434\n",
            "Loss: 2.1290947632296184\n",
            "Loss: 2.1166774098449848\n",
            "Loss: 2.105815619569577\n",
            "Loss: 2.096314520528721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "684b1cae33dcf5cfac008c861984e4ab",
          "grade": false,
          "grade_id": "cell-5141aa398ebc025b",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "DB0he9pMPFwl",
        "colab_type": "text"
      },
      "source": [
        "Let's display the parameter values after the training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "77755b7699f2b508bd93f2ca6ab3faf1",
          "grade": false,
          "grade_id": "cell-95a410ca3fc16ff4",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "CeJwja8TPFwl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48605124-e7e8-4861-8c24-a3453fe761c6"
      },
      "source": [
        "print(\"a:\", va.value, \"b:\", vb.value, \"c:\", vc.value)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: 0.2676897030023671 b: 0.09446139849938508 c: 1.9725828646266883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "48b8bb0f97cad4c99d7984693038c2fd",
          "grade": false,
          "grade_id": "cell-3047b41fc21ba7d6",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "V37TOYEVPFwn",
        "colab_type": "text"
      },
      "source": [
        "Let's display the points, along with the fitted parabola."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "9e4f0bcdcb18aa9d5e727aac10976fc4",
          "grade": false,
          "grade_id": "cell-cc778c65d5a12df7",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "Lp4YsOxUPFwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def plot_points_and_y(points, vx, oy):\n",
        "    fig, ax = plt.subplots()\n",
        "    xs, ys = zip(*points)\n",
        "    ax.plot(xs, ys, 'r+')\n",
        "    x_min, x_max = np.min(xs), np.max(xs)\n",
        "    step = (x_max - x_min) / 100\n",
        "    x_list = list(np.arange(x_min, x_max + step, step))\n",
        "    y_list = []\n",
        "    for x in x_list:\n",
        "        vx.assign(x)\n",
        "        oy.compute()\n",
        "        y_list.append(oy.value)\n",
        "    ax.plot(x_list, y_list)\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.show()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "710c58bc3ba94f9ee5cd315d99421d51",
          "grade": false,
          "grade_id": "cell-d447df911db9b08",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "dc4i17njPFwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "961f382b-a6c2-4a89-9afb-eeb408de53f7"
      },
      "source": [
        "plot_points_and_y(points, vx, oy)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1907c945cfa5f2e4638c61f3560d54de",
          "grade": false,
          "grade_id": "cell-7e6680f77cb76c7d",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "5FTKDblzPFws",
        "colab_type": "text"
      },
      "source": [
        "This looks like a good fit!\n",
        "\n",
        "Note that if we chose too large a learning step, we would not converge to a solution.  A large step causes the parameter values to zoom all over the place, possibly missing by large amounts the (local) minima where you want to converge.  In the limit where the step size goes to 0, and the number of steps to infinity, you are guaranteed (if the function is differentiable, and some other hypotheses) converge to the minimum; the problem is that it would take infinitely long.  You will learn in a more in-depth ML class how to tune the step size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1961402bee68ea5316d93e86260cb473",
          "grade": false,
          "grade_id": "cell-5746cf6c31405c43",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "oYb2xmXsPFwt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b1f0a311-8066-423a-f998-331114e6b7c9"
      },
      "source": [
        "# Let us reinitialize the variables.\n",
        "va.assign(0)\n",
        "vb.assign(0)\n",
        "vc.assign(0)\n",
        "# ... and let's use a big step size.\n",
        "fit(loss, points, [va, vb, vc], delta=0.01, num_iterations=1000)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 7.445536528760376e+257\n",
            "Loss: inf\n",
            "Loss: nan\n",
            "Loss: nan\n",
            "Loss: nan\n",
            "Loss: nan\n",
            "Loss: nan\n",
            "Loss: nan\n",
            "Loss: nan\n",
            "Loss: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "0454c20efb1a32aabff4201d3c837d03",
          "grade": false,
          "grade_id": "cell-ae997060fea51fd5",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "ZKcCJoBZPFwu",
        "colab_type": "text"
      },
      "source": [
        "A step size of 0.01 was enough to take us to infinity and beyond."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b59e739a30ff467161b87abfbeb22b92",
          "grade": false,
          "grade_id": "cell-5bfd0d5cce956f0a",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "dgwWD33VPFwu",
        "colab_type": "text"
      },
      "source": [
        "Let us now show you how to fit a simple linear regression: $y = ax + b$, so $L = (y - (ax + b))^2$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "1bb03b73ba21136869e7a29698474571",
          "grade": false,
          "grade_id": "cell-92e1ad5186390378",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "49SWnid_PFww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "# Sometimes you have to be careful about initial values.\n",
        "va = V(1.)\n",
        "vb = V(1.)\n",
        "\n",
        "# x and y\n",
        "vx = V(0.)\n",
        "vy = V(0.)\n",
        "\n",
        "# Predicted y\n",
        "oy = va * vx + vb\n",
        "\n",
        "# Loss\n",
        "loss = (vy - oy) * (vy - oy)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "f2e913e9f6f50f9ae1f5c045bb135dc3",
          "grade": false,
          "grade_id": "cell-56391868bda5a4dd",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "nnN9CYB9PFwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "4d88917c-b341-4eb3-d411-749badaaeef5"
      },
      "source": [
        "fit(loss, points, [va, vb])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 28.04911931725314\n",
            "Loss: 24.56807140584221\n",
            "Loss: 22.04737547646302\n",
            "Loss: 20.018461554483544\n",
            "Loss: 18.363064879989466\n",
            "Loss: 17.010221159926804\n",
            "Loss: 15.904419410376471\n",
            "Loss: 15.000526654509262\n",
            "Loss: 14.261674191105309\n",
            "Loss: 13.657727388420048\n",
            "Loss: 13.164054060381849\n",
            "Loss: 12.760519585409948\n",
            "Loss: 12.43066568540981\n",
            "Loss: 12.161039167477448\n",
            "Loss: 11.940643231055738\n",
            "Loss: 11.760488960015408\n",
            "Loss: 11.613228706940784\n",
            "Loss: 11.492856417067383\n",
            "Loss: 11.394462669488892\n",
            "Loss: 11.314034444913947\n",
            "Loss: 11.248291453438622\n",
            "Loss: 11.194552346912685\n",
            "Loss: 11.150625359329684\n",
            "Loss: 11.114718914975501\n",
            "Loss: 11.085368558461209\n",
            "Loss: 11.061377226459106\n",
            "Loss: 11.041766425106331\n",
            "Loss: 11.025736321831346\n",
            "Loss: 11.012633123936265\n",
            "Loss: 11.00192241346032\n",
            "Loss: 10.99316735077883\n",
            "Loss: 10.986010857964564\n",
            "Loss: 10.980161055254529\n",
            "Loss: 10.975379356643991\n",
            "Loss: 10.971470739082786\n",
            "Loss: 10.968275788400177\n",
            "Loss: 10.965664197548818\n",
            "Loss: 10.963529451991706\n",
            "Loss: 10.96178448547398\n",
            "Loss: 10.960358128998926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.960358128998926"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "622ab298dfd5c923dadf2aef73fb8230",
          "grade": false,
          "grade_id": "cell-6b17fe17e012350",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "2QT7_o2lPFw2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "541497a6-459f-46ba-c686-97dc83c2682d"
      },
      "source": [
        "plot_points_and_y(points, vx, oy)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "7bf0ba4643a284c16d0a10d295b48214",
          "grade": false,
          "grade_id": "cell-419a81c421892b55",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "nOBHBA0XPFw4",
        "colab_type": "text"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "Using the method illustrated above, fit the following equations to our set of points.  Use `vx`, `xy` for $x$, $y$, and `va`, `vb`, `vc`, etc for the parameters.  This is important, or the tests won't pass.\n",
        "\n",
        "$$\n",
        "y = a^x + bx + c\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "189c7c01abf816b030e876f085210b5a",
          "grade": false,
          "grade_id": "cell-7947daf5b2b9158c",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "mLY5e9JMPFw4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "ea9100ad-7377-4385-e68e-f42000007c08"
      },
      "source": [
        "### Exercise: fit of y = a^x + bx + c\n",
        "\n",
        "vx = V(0.)\n",
        "vy = V(0.)\n",
        "va = V(1.)\n",
        "vb = V(0.)\n",
        "vc = V(0.)\n",
        "# Define below what is oy and loss. \n",
        "oy = va ** vx + vb * vx + vc\n",
        "loss = (vy - oy)*(vy - oy)\n",
        "\n",
        "fit(loss, points, [va, vb, vc])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 13.22031673565597\n",
            "Loss: 11.141654640359056\n",
            "Loss: 9.524337885770317\n",
            "Loss: 8.258639654229485\n",
            "Loss: 7.261959756098997\n",
            "Loss: 6.472240243458916\n",
            "Loss: 5.842777193129943\n",
            "Loss: 5.338271943130829\n",
            "Loss: 4.93188598839005\n",
            "Loss: 4.603069161367653\n",
            "Loss: 4.335966482106359\n",
            "Loss: 4.118250607697591\n",
            "Loss: 3.940264273483409\n",
            "Loss: 3.794387517804188\n",
            "Loss: 3.6745678050846706\n",
            "Loss: 3.575968472470752\n",
            "Loss: 3.494703513553483\n",
            "Loss: 3.427635753770303\n",
            "Loss: 3.3722219198375187\n",
            "Loss: 3.326392689534276\n",
            "Loss: 3.288459066630091\n",
            "Loss: 3.2570387471581963\n",
            "Loss: 3.2309978039038927\n",
            "Loss: 3.2094042107284664\n",
            "Loss: 3.191490593868335\n",
            "Loss: 3.1766242293232927\n",
            "Loss: 3.164282770875983\n",
            "Loss: 3.1540345391718048\n",
            "Loss: 3.1455224617311255\n",
            "Loss: 3.1384509501803577\n",
            "Loss: 3.1325751510442505\n",
            "Loss: 3.127692122085746\n",
            "Loss: 3.1236335760432823\n",
            "Loss: 3.120259903985071\n",
            "Loss: 3.117455245995913\n",
            "Loss: 3.1151234209610212\n",
            "Loss: 3.113184562377402\n",
            "Loss: 3.111572335343445\n",
            "Loss: 3.1102316326277197\n",
            "Loss: 3.109116666132855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.109116666132855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "c3d77a2c13a27ec57ecb01a327aa416d",
          "grade": true,
          "grade_id": "cell-c3490df02b98346d",
          "locked": true,
          "points": 15,
          "schema_version": 1,
          "solution": false
        },
        "id": "hhsUAw9oPFw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Tests for convergence of fit of y = a^x + bx + c\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "563c810a5a0e33fd176a8c09482302bd",
          "grade": false,
          "grade_id": "cell-e73fe61cd5ddeadf",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "juYAm0z6PFw7",
        "colab_type": "text"
      },
      "source": [
        "Now, fit: \n",
        "\n",
        "$$\n",
        "y = a \\cdot 2^x + b \\cdot 2^{-x} + c x^3 + d x^2 + e x + f\n",
        "$$\n",
        "\n",
        "Use a small enough step size, and a sufficient number of iterations, to obtain a final loss of no more than 2.5.\n",
        "\n",
        "Hint: write `vx * vx * vx`, not `vx ** 3`, etc, since as currently written, the `**` operator cannot handle a negative basis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "3f01b8d68f50271c58022db4a1645992",
          "grade": false,
          "grade_id": "cell-fe4eb30e192f5203",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "id": "TKhDylZOPFw8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cf7a1461-7c44-4374-a764-7d05e94c3cf4"
      },
      "source": [
        "### Exercise: fit of y = a 2^x + b 2^{-x} + c x^3 + d x^2 + e x + f\n",
        "vx = V(0.)\n",
        "vy = V(0.)\n",
        "va = V(1.)\n",
        "vb = V(1.)\n",
        "vc = V(0.)\n",
        "vd = V(0.)\n",
        "ve = V(0.)\n",
        "vf = V(0.)\n",
        "# Define here what is oy and what is the loss.\n",
        "oy = va* (2**vx)+vb*(2**-vx)+vc*(vx*vx*vx)+vd*(vx*vx)+ve*vx+vf\n",
        "loss = (vy - oy)*(vy - oy)\n",
        "fit(loss, points, [va, vb, vc, vd, ve, vf], delta=0.00002, num_iterations=35000)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 14.47308263741464\n",
            "Loss: 13.149118394564304\n",
            "Loss: 12.052269282606229\n",
            "Loss: 11.119822299760546\n",
            "Loss: 10.31298951008421\n",
            "Loss: 9.606610841909317\n",
            "Loss: 8.983438928103547\n",
            "Loss: 8.43094997378422\n",
            "Loss: 7.939549077723112\n",
            "Loss: 7.501547014615207\n",
            "Loss: 7.110565303579981\n",
            "Loss: 6.761180362550861\n",
            "Loss: 6.448702292740461\n",
            "Loss: 6.169030495014693\n",
            "Loss: 5.918554022346396\n",
            "Loss: 5.694078743762034\n",
            "Loss: 5.492771219838277\n",
            "Loss: 5.3121135201238765\n",
            "Loss: 5.149865618176149\n",
            "Loss: 5.004033343775038\n",
            "Loss: 4.872840629612138\n",
            "Loss: 4.7547052230493065\n",
            "Loss: 4.64821728657702\n",
            "Loss: 4.552120462878798\n",
            "Loss: 4.465295075834541\n",
            "Loss: 4.386743201678053\n",
            "Loss: 4.315575388373977\n",
            "Loss: 4.2509988336505025\n",
            "Loss: 4.192306857282233\n",
            "Loss: 4.138869523607626\n",
            "Loss: 4.0901252873179\n",
            "Loss: 4.045573550136237\n",
            "Loss: 4.0047680286611715\n",
            "Loss: 3.9673108447394214\n",
            "Loss: 3.9328472595138773\n",
            "Loss: 3.901060980951364\n",
            "Loss: 3.8716699823395846\n",
            "Loss: 3.8444227760730754\n",
            "Loss: 3.819095093124992\n",
            "Loss: 3.795486924011447\n",
            "Loss: 3.7734198818726346\n",
            "Loss: 3.7527348525863715\n",
            "Loss: 3.7332899006523883\n",
            "Loss: 3.714958402991834\n",
            "Loss: 3.697627385840947\n",
            "Loss: 3.681196042621845\n",
            "Loss: 3.665574413082624\n",
            "Loss: 3.650682206145671\n",
            "Loss: 3.636447750816092\n",
            "Loss: 3.622807061206376\n",
            "Loss: 3.609703003252583\n",
            "Loss: 3.597084552050231\n",
            "Loss: 3.584906129944353\n",
            "Loss: 3.5731270165824127\n",
            "Loss: 3.5617108230965164\n",
            "Loss: 3.5506250234343013\n",
            "Loss: 3.539840536618276\n",
            "Loss: 3.5293313543907447\n",
            "Loss: 3.5190742093051206\n",
            "Loss: 3.509048278862245\n",
            "Loss: 3.4992349217697\n",
            "Loss: 3.4896174428291054\n",
            "Loss: 3.4801808833370407\n",
            "Loss: 3.4709118342242493\n",
            "Loss: 3.46179826945999\n",
            "Loss: 3.452829397517759\n",
            "Loss: 3.4439955289382973\n",
            "Loss: 3.4352879582399214\n",
            "Loss: 3.426698858616403\n",
            "Loss: 3.418221188032683\n",
            "Loss: 3.409848605479737\n",
            "Loss: 3.4015753962848683\n",
            "Loss: 3.3933964054938053\n",
            "Loss: 3.385306978447903\n",
            "Loss: 3.3773029077753236\n",
            "Loss: 3.3693803860998823\n",
            "Loss: 3.3615359638471314\n",
            "Loss: 3.3537665115946473\n",
            "Loss: 3.346069186473691\n",
            "Loss: 3.3384414021830136\n",
            "Loss: 3.3308808022232523\n",
            "Loss: 3.3233852360031104\n",
            "Loss: 3.315952737506194\n",
            "Loss: 3.308581506241419\n",
            "Loss: 3.301269890229854\n",
            "Loss: 3.2940163708078227\n",
            "Loss: 3.286819549049906\n",
            "Loss: 3.279678133636912\n",
            "Loss: 3.272590930012802\n",
            "Loss: 3.265556830691514\n",
            "Loss: 3.258574806589725\n",
            "Loss: 3.2516438992750656\n",
            "Loss: 3.2447632140312317\n",
            "Loss: 3.2379319136521105\n",
            "Loss: 3.2311492128867036\n",
            "Loss: 3.2244143734648905\n",
            "Loss: 3.217726699641839\n",
            "Loss: 3.2110855342054725\n",
            "Loss: 3.2044902548975207\n",
            "Loss: 3.19794027120394\n",
            "Loss: 3.1914350214753244\n",
            "Loss: 3.184973970342192\n",
            "Loss: 3.1785566063937387\n",
            "Loss: 3.172182440092163\n",
            "Loss: 3.165851001897576\n",
            "Loss: 3.1595618405812234\n",
            "Loss: 3.1533145217071974\n",
            "Loss: 3.1471086262648593\n",
            "Loss: 3.14094374943618\n",
            "Loss: 3.13481949948386\n",
            "Loss: 3.128735496747626\n",
            "Loss: 3.122691372737431\n",
            "Loss: 3.116686769313511\n",
            "Loss: 3.1107213379443097\n",
            "Loss: 3.1047947390342503\n",
            "Loss: 3.0989066413141826\n",
            "Loss: 3.0930567212881055\n",
            "Loss: 3.087244662730419\n",
            "Loss: 3.081470156228613\n",
            "Loss: 3.0757328987667814\n",
            "Loss: 3.070032593345904\n",
            "Loss: 3.0643689486371812\n",
            "Loss: 3.058741678665195\n",
            "Loss: 3.05315050251793\n",
            "Loss: 3.047595144081015\n",
            "Loss: 3.0420753317938614\n",
            "Loss: 3.0365907984255656\n",
            "Loss: 3.031141280868681\n",
            "Loss: 3.0257265199491656\n",
            "Loss: 3.0203462602510087\n",
            "Loss: 3.015000249954104\n",
            "Loss: 3.00968824068422\n",
            "Loss: 3.004409987373906\n",
            "Loss: 2.9991652481333535\n",
            "Loss: 2.9939537841303645\n",
            "Loss: 2.988775359478528\n",
            "Loss: 2.983629741133002\n",
            "Loss: 2.9785166987931415\n",
            "Loss: 2.9734360048114428\n",
            "Loss: 2.9683874341082706\n",
            "Loss: 2.9633707640918647\n",
            "Loss: 2.9583857745831894\n",
            "Loss: 2.9534322477452792\n",
            "Loss: 2.9485099680166758\n",
            "Loss: 2.943618722048656\n",
            "Loss: 2.9387582986459453\n",
            "Loss: 2.9339284887106793\n",
            "Loss: 2.9291290851893383\n",
            "Loss: 2.9243598830224595\n",
            "Loss: 2.9196206790969073\n",
            "Loss: 2.914911272200541\n",
            "Loss: 2.9102314629790897\n",
            "Loss: 2.9055810538951143\n",
            "Loss: 2.900959849188866\n",
            "Loss: 2.8963676548409687\n",
            "Loss: 2.8918042785367826\n",
            "Loss: 2.8872695296323214\n",
            "Loss: 2.8827632191216788\n",
            "Loss: 2.878285159605797\n",
            "Loss: 2.873835165262587\n",
            "Loss: 2.8694130518182184\n",
            "Loss: 2.8650186365195935\n",
            "Loss: 2.860651738107902\n",
            "Loss: 2.8563121767931863\n",
            "Loss: 2.851999774229885\n",
            "Loss: 2.847714353493272\n",
            "Loss: 2.8434557390567816\n",
            "Loss: 2.839223756770136\n",
            "Loss: 2.8350182338382464\n",
            "Loss: 2.8308389988008416\n",
            "Loss: 2.8266858815128217\n",
            "Loss: 2.822558713125245\n",
            "Loss: 2.8184573260669428\n",
            "Loss: 2.81438155402675\n",
            "Loss: 2.8103312319362956\n",
            "Loss: 2.8063061959533377\n",
            "Loss: 2.802306283445595\n",
            "Loss: 2.7983313329751107\n",
            "Loss: 2.794381184283042\n",
            "Loss: 2.790455678274931\n",
            "Loss: 2.7865546570063713\n",
            "Loss: 2.782677963669122\n",
            "Loss: 2.7788254425775807\n",
            "Loss: 2.7749969391556313\n",
            "Loss: 2.771192299923879\n",
            "Loss: 2.767411372487191\n",
            "Loss: 2.76365400552258\n",
            "Loss: 2.759920048767405\n",
            "Loss: 2.7562093530078737\n",
            "Loss: 2.7525217700678133\n",
            "Loss: 2.7488571527977483\n",
            "Loss: 2.74521535506421\n",
            "Loss: 2.7415962317393308\n",
            "Loss: 2.7379996386906655\n",
            "Loss: 2.7344254327712454\n",
            "Loss: 2.730873471809864\n",
            "Loss: 2.7273436146015864\n",
            "Loss: 2.7238357208984447\n",
            "Loss: 2.720349651400352\n",
            "Loss: 2.7168852677462128\n",
            "Loss: 2.71344243250519\n",
            "Loss: 2.710021009168188\n",
            "Loss: 2.706620862139481\n",
            "Loss: 2.703241856728505\n",
            "Loss: 2.699883859141837\n",
            "Loss: 2.6965467364752986\n",
            "Loss: 2.6932303567062092\n",
            "Loss: 2.6899345886858117\n",
            "Loss: 2.6866593021317913\n",
            "Loss: 2.6834043676209665\n",
            "Loss: 2.6801696565820876\n",
            "Loss: 2.676955041288754\n",
            "Loss: 2.6737603948524806\n",
            "Loss: 2.6705855912158345\n",
            "Loss: 2.66743050514573\n",
            "Loss: 2.664295012226792\n",
            "Loss: 2.661178988854857\n",
            "Loss: 2.6580823122305466\n",
            "Loss: 2.6550048603529564\n",
            "Loss: 2.6519465120134353\n",
            "Loss: 2.6489071467894534\n",
            "Loss: 2.645886645038567\n",
            "Loss: 2.642884887892451\n",
            "Loss: 2.639901757251034\n",
            "Loss: 2.636937135776714\n",
            "Loss: 2.6339909068886342\n",
            "Loss: 2.6310629547570503\n",
            "Loss: 2.6281531642977685\n",
            "Loss: 2.6252614211666514\n",
            "Loss: 2.6223876117542084\n",
            "Loss: 2.619531623180216\n",
            "Loss: 2.616693343288462\n",
            "Loss: 2.613872660641501\n",
            "Loss: 2.611069464515488\n",
            "Loss: 2.6082836448950992\n",
            "Loss: 2.60551509246847\n",
            "Loss: 2.6027636986222187\n",
            "Loss: 2.6000293554365164\n",
            "Loss: 2.597311955680215\n",
            "Loss: 2.5946113928060206\n",
            "Loss: 2.591927560945733\n",
            "Loss: 2.5892603549055146\n",
            "Loss: 2.586609670161233\n",
            "Loss: 2.583975402853836\n",
            "Loss: 2.5813574497847775\n",
            "Loss: 2.578755708411488\n",
            "Loss: 2.5761700768428963\n",
            "Loss: 2.573600453834985\n",
            "Loss: 2.5710467387864\n",
            "Loss: 2.568508831734096\n",
            "Loss: 2.565986633349015\n",
            "Loss: 2.5634800449318353\n",
            "Loss: 2.5609889684087133\n",
            "Loss: 2.5585133063271104\n",
            "Loss: 2.5560529618516212\n",
            "Loss: 2.5536078387598704\n",
            "Loss: 2.5511778414384176\n",
            "Loss: 2.5487628748787143\n",
            "Loss: 2.546362844673102\n",
            "Loss: 2.543977657010821\n",
            "Loss: 2.5416072186740752\n",
            "Loss: 2.539251437034124\n",
            "Loss: 2.5369102200474054\n",
            "Loss: 2.5345834762516812\n",
            "Loss: 2.5322711147622403\n",
            "Loss: 2.529973045268103\n",
            "Loss: 2.5276891780282797\n",
            "Loss: 2.525419423868038\n",
            "Loss: 2.523163694175225\n",
            "Loss: 2.5209219008965937\n",
            "Loss: 2.5186939565341713\n",
            "Loss: 2.5164797741416605\n",
            "Loss: 2.514279267320849\n",
            "Loss: 2.5120923502180768\n",
            "Loss: 2.5099189375206965\n",
            "Loss: 2.5077589444535846\n",
            "Loss: 2.5056122867756705\n",
            "Loss: 2.503478880776498\n",
            "Loss: 2.5013586432727934\n",
            "Loss: 2.4992514916050834\n",
            "Loss: 2.497157343634328\n",
            "Loss: 2.495076117738566\n",
            "Loss: 2.49300773280961\n",
            "Loss: 2.490952108249739\n",
            "Loss: 2.488909163968434\n",
            "Loss: 2.4868788203791294\n",
            "Loss: 2.4848609983959897\n",
            "Loss: 2.4828556194306977\n",
            "Loss: 2.480862605389293\n",
            "Loss: 2.4788818786690014\n",
            "Loss: 2.4769133621551047\n",
            "Loss: 2.4749569792178336\n",
            "Loss: 2.473012653709263\n",
            "Loss: 2.471080309960259\n",
            "Loss: 2.469159872777423\n",
            "Loss: 2.467251267440067\n",
            "Loss: 2.4653544196971953\n",
            "Loss: 2.4634692557645446\n",
            "Loss: 2.4615957023215946\n",
            "Loss: 2.459733686508636\n",
            "Loss: 2.457883135923834\n",
            "Loss: 2.456043978620342\n",
            "Loss: 2.454216143103398\n",
            "Loss: 2.4523995583274694\n",
            "Loss: 2.450594153693392\n",
            "Loss: 2.448799859045571\n",
            "Loss: 2.4470166046691357\n",
            "Loss: 2.4452443212871766\n",
            "Loss: 2.44348294005796\n",
            "Loss: 2.4417323925721846\n",
            "Loss: 2.4399926108502377\n",
            "Loss: 2.4382635273394837\n",
            "Loss: 2.436545074911562\n",
            "Loss: 2.4348371868597107\n",
            "Loss: 2.4331397968961\n",
            "Loss: 2.431452839149183\n",
            "Loss: 2.429776248161077\n",
            "Loss: 2.428109958884943\n",
            "Loss: 2.42645390668239\n",
            "Loss: 2.4248080273209087\n",
            "Loss: 2.4231722569713052\n",
            "Loss: 2.4215465322051495\n",
            "Loss: 2.4199307899922635\n",
            "Loss: 2.4183249676981946\n",
            "Loss: 2.4167290030817346\n",
            "Loss: 2.4151428342924297\n",
            "Loss: 2.4135663998681247\n",
            "Loss: 2.4119996387325218\n",
            "Loss: 2.4104424901927324\n",
            "Loss: 2.4088948939368815\n",
            "Loss: 2.4073567900316952\n",
            "Loss: 2.4058281189201236\n",
            "Loss: 2.404308821418966\n",
            "Loss: 2.4027988387165267\n",
            "Loss: 2.401298112370269\n",
            "Loss: 2.3998065843044922\n",
            "Loss: 2.3983241968080233\n",
            "Loss: 2.396850892531929\n",
            "Loss: 2.3953866144872307\n",
            "Loss: 2.3939313060426417\n",
            "Loss: 2.392484910922314\n",
            "Loss: 2.391047373203612\n",
            "Loss: 2.38961863731488\n",
            "Loss: 2.3881986480332444\n",
            "Loss: 2.386787350482414\n",
            "Loss: 2.385384690130498\n",
            "Loss: 2.383990612787859\n",
            "Loss: 2.3826050646049337\n",
            "Loss: 2.38122799207012\n",
            "Loss: 2.379859342007634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.379859342007634"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "07abf8ded4d39d4d00c30e2313f83e8e",
          "grade": true,
          "grade_id": "cell-6699821f3ae13756",
          "locked": true,
          "points": 15,
          "schema_version": 1,
          "solution": false
        },
        "id": "pOMDCxfuPFw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Tests for fit of  y = a 2^x + b 2^{-x} + c x^3 + d x^2 + e x + f\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "831de0d8b55e8de3d545e906608eba07",
          "grade": false,
          "grade_id": "cell-81ed3f93e76881c1",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "BgZ-nL40PFxA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "6c2239bc-006f-4411-d158-b80d4a860d2d"
      },
      "source": [
        "plot_points_and_y(points, vx, oy)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}